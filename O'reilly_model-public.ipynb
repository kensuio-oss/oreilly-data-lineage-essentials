{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling phase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will create a model to be sent to the production team. This model aims to evaluate the churn probability of a customer given several details. The data set used is freely inspired by the Kaggle dataset ([original version](https://www.kaggle.com/blastchar/telco-customer-churn?select=WA_Fn-UseC_-Telco-Customer-Churn.csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements we will track are the following:\n",
    "\n",
    "* The context of the notebook\n",
    "* The data sources information, including their file location, schema and quality metrics\n",
    "* The lineages among those data sources\n",
    "* The models trained and metrics about them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needed libraries and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use the Kensu public library which will allow the tracing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kensu_public import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train our model on a moving window of time to see if the results are reliable over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation of an iteration on the churn model every year\n",
    "from os import remove\n",
    "from os import path\n",
    "iter_file_name = \"iter.txt\"\n",
    "global_fraction = 50\n",
    "iter_fraction = 5\n",
    "if not path.exists(iter_file_name):\n",
    "    with open(iter_file_name, \"w\") as iter_file:\n",
    "        iter_file.write(\"0\")\n",
    "else:\n",
    "    with open(iter_file_name, \"r\") as iter_file:\n",
    "        current_iter = int(iter_file.readline()) + 1\n",
    "        global_fraction = global_fraction + current_iter * iter_fraction\n",
    "    remove(iter_file_name)\n",
    "    if global_fraction < 100:\n",
    "        with open(iter_file_name, \"w\") as iter_file:\n",
    "            iter_file.write(str(current_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a context to our notebook such as the process name, the project in which it fits, and the environment where the notebook is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_name = \"churn_train_12-24\"\n",
    "project_name = \"Churn New Customers\"\n",
    "environment = \"Lab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a part of the dataset at each iteration. We will take about 10 years of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('customer_data.csv', parse_dates=[\"date\"],index_col='Unnamed: 0')\n",
    "\n",
    "\n",
    "data_range_min = int(df.shape[0]*(global_fraction-50)/100)\n",
    "data_range_max = int(df.shape[0]*global_fraction/100)\n",
    "df = df.iloc[data_range_min:data_range_max]\n",
    "timestamp = max(df.date)\n",
    "global current_runtime\n",
    "current_runtime = int(timestamp.timestamp()*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are reporting the context of the notebook, which involves the timestamp, project name, process name, and environment of the script. You can find all this information in the file `oreilly.log` which is created in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_context(current_runtime,process_name,project_name,environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also reporting the data source metadata, such as its format and its schema, to the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_data_source(df,'customer_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if some customers have churned in our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the number of customers that churned\n",
    "df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the count of customer churn\n",
    "sns.countplot(df['Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What percentage of customers are leaving ?\n",
    "retained = df[df.Churn == 'No']\n",
    "churned = df[df.Churn == 'Yes']\n",
    "num_retained = retained.shape[0]\n",
    "num_churned = churned.shape[0]\n",
    "#Print the percentage of customers that stayed and left\n",
    "print( num_retained / (num_retained + num_churned) * 100 , \"% of customers stayed with the company.\")\n",
    "#Print the percentage of customers that stayed and left\n",
    "print( num_churned / (num_retained + num_churned) * 100,\"% of customers left the company.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we see a pattern in the churn rate in function of the gender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the churn count for both Males and Females\n",
    "sns.countplot(x='gender', hue='Churn',data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore if we see a pattern in the churn rate vs. the tenure (Number of months the customer has stayed with the company) or the MonthlyCharges (The amount charged to the customer monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['tenure', 'MonthlyCharges']\n",
    "fig, ax = plt.subplots(1, 2, figsize=(28, 8))\n",
    "df[df.Churn == 'No'][numerical_features].hist(bins=20, color=\"blue\", alpha=0.5, ax=ax)\n",
    "df[df.Churn == 'Yes'][numerical_features].hist(bins=20, color=\"orange\", alpha=0.5, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the unnecessary columns: customerID and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df = df.drop(['customerID','date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert all the non-numeric columns to numerical data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in cleaned_df.columns:\n",
    "    if cleaned_df[column].dtype == np.number:\n",
    "        continue\n",
    "        \n",
    "    cleaned_df[column] = LabelEncoder().fit_transform(cleaned_df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now saving the dataset into a csv file. As we create a new file in the filesystem, we must registered its provenance (its lineage). The file `cleaned_data.csv` is created from the file `customer_data` where we deleted the columns date and customerID. For all those data sources, we will send metadata in the context of the process, such as data statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_data_source(df,'cleaned_data.csv')\n",
    "report_link(['customer_data'],'cleaned_data',current_runtime=current_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the cleaned data, we can now create the feature matrix X and the target Y. Then, we split the data in a train and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_df.drop('Churn', axis = 1) \n",
    "X.to_csv('Xmatrix.csv')\n",
    "report_data_source(X,'Xmatrix.csv')\n",
    "report_link(['cleaned_data'],'Xmatrix',current_runtime=current_runtime)\n",
    "y = cleaned_df['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "report_data_source(x_train,'Xtrain.csv')\n",
    "report_link(['Xmatrix'],'Xtrain',current_runtime=current_runtime)\n",
    "report_data_source(x_test,'Xtest.csv')\n",
    "report_link(['Xmatrix'],'Xtest',current_runtime=current_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model\n",
    "model = LogisticRegression(max_iter=len(x_train))\n",
    "#Train the model\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are saving the model as a joblib image. As we save an element, we must register it in the data lineage. The metadata of a model imply its schema, hyperparameters and performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(model, 'logisticreg.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_model(\"Xtrain\",\"Xtest\",x_test,y_test,model,'logisticreg.joblib',current_runtime=current_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same with a Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(model, 'randomforest.joblib') \n",
    "report_model(\"Xtrain\",\"Xtest\",x_test,y_test,model,'randomforest.joblib',current_runtime=current_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the created logs in the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat oreilly.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
